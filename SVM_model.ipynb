{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import audb\n",
    "import audiofile\n",
    "from scipy.io.wavfile import read as read_wav\n",
    "from matplotlib import pyplot as plt\n",
    "import opensmile\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split #train/test split \n",
    "from sklearn.svm import SVC #Model\n",
    "from sklearn.metrics import confusion_matrix #Metrics \n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.model_selection import cross_val_score #Cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import scipy\n",
    "import random #for shuffling values \n",
    "from scipy import stats\n",
    "\n",
    "#List of prosodic features we will observe\n",
    "\n",
    "features_prosody= ['Loudness_sma3', \n",
    "                   'hammarbergIndex_sma3',\n",
    "                   'F0semitoneFrom27.5Hz_sma3nz',\n",
    "                   'jitterLocal_sma3nz',\n",
    " 'shimmerLocaldB_sma3nz',\n",
    " 'HNRdBACF_sma3nz']\n",
    "\n",
    "\n",
    "#Gets low-level features of all audios \n",
    "def lowlevel_features(list_with_files, dir_path):\n",
    "    \n",
    "    path = dir_path\n",
    "    #  Define feature extractor\n",
    "    smile = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
    "    feature_level=opensmile.FeatureLevel.LowLevelDescriptors,\n",
    "    )\n",
    "\n",
    "    df_data= []\n",
    "\n",
    "    for i in tqdm((list_with_files)):\n",
    "        f= path + i\n",
    "        sampling_rate, data = read_wav(f) #gets sampling rate and dimensions of data\n",
    "        #print(f\"Sampling rate of {f}:{sampling_rate}\")\n",
    "        \n",
    "        db = audb.load('emodb',\n",
    "        version='1.1.1',\n",
    "        format='wav',\n",
    "        mixdown=True,\n",
    "        sampling_rate=sampling_rate,\n",
    "        media='wav/03a01.*',  # load subset\n",
    "        full_path=False,\n",
    "        verbose=False,)\n",
    "\n",
    "        #extract features\n",
    "        signal, sampling_rate = audiofile.read(f, duration=120, always_2d=True)\n",
    "        data= smile.process_signal(signal, sampling_rate)\n",
    "        df= data.loc[:,features_prosody]\n",
    "        # Select the column you want to normalize\n",
    "        column_to_normalize = 'F0semitoneFrom27.5Hz_sma3nz'\n",
    "        # Extract the column as a numpy array\n",
    "        column_data = df[column_to_normalize].values.reshape(-1, 1)\n",
    "        # Normalize the column\n",
    "        scaler = StandardScaler()\n",
    "        normalized_column = scaler.fit_transform(column_data)\n",
    "        # Replace the original column with the normalized values\n",
    "        df[column_to_normalize] = normalized_column\n",
    "        df_data.append(df.values)\n",
    "\n",
    "   # df= pd.concat(df_data, ignore_index=True)\n",
    "    return df_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all labels \n",
    "\n",
    "labels= open('labels_extension.txt', 'r').read().split('\\n')\n",
    "labels= [int(label) for label in labels if label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065694b699c54ec195c9507e42b0ee8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/541 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Process wav files \n",
    "path=\"audios/\"\n",
    "dir_list = os.listdir(path)\n",
    "feature_data= lowlevel_features(dir_list, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the shape of the largest matrix\n",
    "max_shape = max(matrix.shape for matrix in feature_data)\n",
    "\n",
    "# Zero-pad the rows of matrices\n",
    "padded_matrices = []\n",
    "for matrix in feature_data:\n",
    "    pad_width = ((0, max_shape[0] - matrix.shape[0]), (0, 0))\n",
    "    padded_matrix = np.pad(matrix, pad_width, mode='constant', constant_values=0)\n",
    "    padded_matrices.append(padded_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Test split \n",
    "y= labels\n",
    "#y= y_axis.to_numpy().flatten() #Labels (class 0 and 1)\n",
    "X = np.asarray(padded_matrices)\n",
    "X= X.reshape(-1, X.shape[1]*X.shape[2])\n",
    "#X= X_axis.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea262615fab48fbb9fa578085ae259c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max. recall:1.0\n",
      "Optimal C: 1.8329807108324339\n",
      "Optimal gamma:1.1288378916846883e-06\n"
     ]
    }
   ],
   "source": [
    "#Cross-validation (Overfitting)\n",
    "\n",
    "C_list= np.logspace(-5, 5, 20)\n",
    "gamma_list= np.logspace(-8, -5, 20)\n",
    "recall = np.zeros((len(C_list), len(gamma_list)))\n",
    "precision = np.zeros((len(C_list), len(gamma_list)))\n",
    "\n",
    "for i in tqdm(range(len(C_list))):\n",
    "     for j in range(len(gamma_list)):\n",
    "         svc= SVC(C=C_list[i], gamma=gamma_list[j], kernel='rbf')\n",
    "         svc.fit(X_train, y_train)\n",
    "         y_pred= svc.predict(X_train)\n",
    "         tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "         recall[i, j] = float(tp) / (tp + fn)\n",
    "         precision[i,j]= float(tp + tn) / (tp + tn + fp + fn)\n",
    "               \n",
    "\n",
    "index_max = np.unravel_index(np.argmax(recall), recall.shape)\n",
    "\n",
    "print('Max. recall:{}'.format(recall[index_max]))\n",
    "C_opt = C_list[index_max[0]]\n",
    "gamma_opt = gamma_list[index_max[1]]\n",
    "print('Optimal C: {}'.format(C_opt))\n",
    "print('Optimal gamma:{}'.format(gamma_opt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal hyperparameters for overfitting \n",
    "C_opt_of=1.8329807108324339\n",
    "gamma_opt_of=1.1288378916846883e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.9681528662420382\n",
      "Precision 0.9855072463768116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[188,   0],\n",
       "       [  5, 152]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc= SVC(C=C_opt_of, gamma=gamma_opt_of, kernel='rbf')\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred= svc.predict(X_train)\n",
    "cm= confusion_matrix(y_train, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "recall = float(tp) / (tp + fn)\n",
    "precision= float(tp + tn) / (tp + fn + fp + tn)\n",
    "\n",
    "print('Recall: {}'.format(recall)) \n",
    "print('Precision {}'.format(precision))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall mean: 0.3908740065601363\n",
      "Recall standard deviation: 0.1235022529699947\n",
      "Precision mean: 0.5157239057239057\n",
      "Precision standard deviation: 0.07196808039612361\n"
     ]
    }
   ],
   "source": [
    "#Validation of model \n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "recall_scores = []\n",
    "precision_scores=[]\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    X_train_k, y_train_k = X[train_idx, :], np.asarray(y)[train_idx]\n",
    "    X_val_k, y_val_k = X[val_idx, :], np.asarray(y)[val_idx]\n",
    "    svc= SVC(C=C_opt_of, gamma=gamma_opt_of, kernel='rbf', class_weight='balanced')\n",
    "    svc.fit(X_train_k, y_train_k)\n",
    "    y_pred_k= svc.predict(X_val_k)\n",
    "    cm= confusion_matrix(y_val_k, y_pred_k)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val_k, y_pred_k).ravel()\n",
    "    recall_v = float(tp) / (tp + fn)\n",
    "    precision_v= float(tp + tn) / (tp + fn + fp + tn)\n",
    "    recall_scores.append(recall_v)\n",
    "    precision_scores.append(precision_v)\n",
    "\n",
    "print('Recall mean: {}'.format(np.mean(recall_scores))) \n",
    "print('Recall standard deviation: {}'.format(np.std(recall_scores)))\n",
    "print('Precision mean: {}'.format(np.mean(precision_scores))) \n",
    "print('Precision standard deviation: {}'.format(np.std(precision_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validation (classifier)\n",
    "\n",
    "C_list= np.logspace(-5, 5, 20)\n",
    "gamma_list= np.logspace(-8, -5)\n",
    "recall = np.zeros((len(C_list), len(gamma_list)))\n",
    "precision = np.zeros((len(C_list), len(gamma_list)))\n",
    "\n",
    "for i in tqdm(range(len(C_list))):\n",
    "     for j in range(len(gamma_list)):\n",
    "        svc= SVC(C=C_list[i], gamma=gamma_list[j], kernel='rbf')\n",
    "        svc.fit(X_train, y_train)\n",
    "        y_pred= svc.predict(X_train)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "        recall[i, j] = float(tp) / (tp + fn)\n",
    "        precision[i,j]= float(tp + tn) / (tp + tn + fp + fn)\n",
    "               \n",
    "\n",
    "index_max = np.unravel_index(np.argmax(recall), recall.shape)\n",
    "\n",
    "print('Max. recall:{}'.format(recall[index_max]))\n",
    "C_opt = C_list[index_max[0]]\n",
    "gamma_opt = gamma_list[index_max[1]]\n",
    "print('Optimal C: {}'.format(C_opt))\n",
    "print('Optimal gamma:{}'.format(gamma_opt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal parameters from Classifier implementation\n",
    "C_opt= 20.6913808111479\n",
    "gamma_opt= 2.2229964825261955e-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall mean: 0.4343354612376351\n",
      "Recall standard deviation: 0.04092508870371035\n",
      "Precision mean: 0.5065319865319866\n",
      "Precision standard deviation: 0.06675014329566945\n"
     ]
    }
   ],
   "source": [
    "#Validation of model \n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "recall_scores = []\n",
    "precision_scores=[]\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    X_train_k, y_train_k = X[train_idx, :], np.asarray(y)[train_idx]\n",
    "    X_val_k, y_val_k = X[val_idx, :], np.asarray(y)[val_idx]\n",
    "    svc= SVC(C=C_opt, gamma=gamma_opt, kernel='rbf', class_weight='balanced')\n",
    "    svc.fit(X_train_k, y_train_k)\n",
    "    y_pred_k= svc.predict(X_val_k)\n",
    "    cm= confusion_matrix(y_val_k, y_pred_k)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val_k, y_pred_k).ravel()\n",
    "    recall_v = float(tp) / (tp + fn)\n",
    "    precision_v= float(tp + tn) / (tp + fn + fp + tn)\n",
    "    recall_scores.append(recall_v)\n",
    "    precision_scores.append(precision_v)\n",
    "\n",
    "print('Recall mean: {}'.format(np.mean(recall_scores))) \n",
    "print('Recall standard deviation: {}'.format(np.std(recall_scores)))\n",
    "print('Precision mean: {}'.format(np.mean(precision_scores))) \n",
    "print('Precision standard deviation: {}'.format(np.std(precision_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33, 24],\n",
       "       [32, 20]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc= SVC(kernel='rbf', class_weight='balanced', C=C_opt, gamma=gamma_opt)\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
